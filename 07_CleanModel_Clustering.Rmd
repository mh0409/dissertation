---
title: "07_CleanModel_Clustering"
author: "Maria Jose Herrera"
date: "7/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Clustering model

# Load data
Note to sef: the data is only looking at mortgage prisoners after 2014 (when regulation was passed).

```{r}
c <- readRDS("/Users/mariajoseherrera/Documents/LSE_new/03_Dissertation/Understanding_Society/df_mtgprisoners_postlegis.rds")
# df_prelegis <- readRDS("/Users/mariajoseherrera/Documents/LSE_new/03_Dissertation/Understanding_Society/df_mtgprisoners_prelegis.rds") # don't need for now

```

# Aux function
```{r}
pct_df <- function(base, higher){(higher - base)/base}

```

# Prepare data

1) Rows are observations (individuals) and columns are variables
2) Any missing value in the data must be removed or estimated.
3) The data must be standardized (i.e., scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one.[^scale]

```{r}
library("dplyr")

# Categorical
cat_vars <- readRDS("/Users/mariajoseherrera/Documents/LSE_new/03_Dissertation/Understanding_Society/dissertation/cat_vars.rds")

# Make sure vars are right data type
df_postlegis[cat_vars] <- lapply(df_postlegis[cat_vars], as.factor)
df_postlegis$wavenumber <- as.factor(df_postlegis$wavenumber)
df_postlegis$sex_dv[df_postlegis$sex_dv == 0] <- NA
df_postlegis$sex_dv <- as.factor(df_postlegis$sex_dv)

# Define variables of interest
vars_of_interest <- readRDS("/Users/mariajoseherrera/Documents/LSE_new/03_Dissertation/Understanding_Society/dissertation/vars_of_interest.rds")
df_interest <- select(df_postlegis, one_of(vars_of_interest)) # create df

# Keep only complete observations
df_interest_complete <- df_interest[complete.cases(df_interest) == TRUE, ]
# unique(df_interest_complete$hsyr04) # no NA data in df_interest_complete
# df_interest_complete <- droplevels(df_interest_complete)

summary(df_interest_complete)

df_clustervars <- df_interest_complete[, !names(df_interest_complete) %in% c("country", "pidp")] # Remove country and PIDP from data frame

```

## One-hot encoding (dummy vars) + scale
```{r}
library("caret")

# dummify the data (make categorical variables into many numeric variables)
dmy <- dummyVars(" ~ .", data = df_clustervars, fullRank = TRUE)
df_dummy <- data.frame(predict(dmy, newdata = df_clustervars))
df_dum_scaled <- scale(df_dummy)

summary(df_dummy)
```
## PCA

```{r}
library("PCAmixdata")

split <- splitmix(df_postlegis[, -52]) # split into quant and qual variables; omit is_mtgprisoner
X1 <- split$X.quanti 
X2 <- split$X.quali 
res.pcamix <- PCAmix(X.quanti=X1, X.quali=X2,rename.level=TRUE,
                     graph=FALSE)

res.pcamix$eig


```

# Dissmilarity matrix
$d_i_j = d(i,j) = sum(k=1:p; w_k delta(ij;k) d(ij,k)) / sum(k=1:p; w_k delta(ij;k))$

## Gower
```{r}
library("cluster")

# Build dissimilarity matrix using gower distribution
gower_dist <- daisy(df_clustervars, metric = c("gower")) # using only vars of interests used in log reg

gower_mat <- as.matrix(gower_dist)

gower_asdist <- as.dist(gower_dist)



```

## Manhattan
* Using df_dum_scaled (one-hot encoding of all categorical vars)
```{r}
onehot_dist <- stats::dist(df_dum_scaled, method = "manhattan")

```


# Hierarchical clustering
```{r}
library("factoextra")
library("ggplot2")
library("purrr")
```



## Agglomerative (agnes)
```{r}
# methods to assess
m <- c( "average", "single", "complete")
names(m) <- c( "average", "single", "complete")

# function to compute coefficient
ac <- function(x) {
  agnes(gower_dist, method = x)$ac
}

map_dbl(m, ac) # complete has highest ac
#  average    single  complete 
# 0.7713831 0.6183628 0.8472675 
```

### Dendogram of agnes -- USE THIS ONE
Source: https://uc-r.github.io/hc_clustering#optimal

```{r}
hc_agnes_ward <- agnes(gower_dist, diss = TRUE, method = "complete")

pdf("/Users/mariajoseherrera/Documents/LSE_new/03_Dissertation/Understanding_Society/agnes_tree.pdf")
pltree(hc_agnes_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 
dev.off()

agnes_as_hclust <- as.hclust(hc_agnes_ward)

```

## Agglomerative (hclust) 
```{r}
# agg_hclust <- hclust(gower_asdist, method = "complete") ## NOT USING THIS ONE BC BAD RESULTS VS AGNES()

# agnes_nbclust <- NbClust(diss =  gower_asdist,
#         distance = NULL,
#         method =  "complete",
#         index = "silhouette")
# agnes_nbclust$Best.nc
# agnes_nbclust$Best.partition
# Result from "average" says to cluster into 2 groups BUT 1 OF 829 AND 1 OF 1 -- THIS IS JUST WRONG

# Cut tree into 2 groups
sub_grp2 <- cutree(as.hclust(hc_agnes_ward), k = 2)

# Number of members in each cluster
table(sub_grp2)
## sub_grp
sub_grp2
##  1   2 
## 525 954 


plot(agg_hclust, hang = -1) # from plot cut into 4?

# Cut tree into 4 groups
sub_grp4 <- cutree(as.hclust(hc_agnes_ward), k = 4) # suggested by PAM (see below)

# Number of members in each cluster
table(sub_grp4)
## sub_grp
## 1   2   3   4 
## 251 612 342 274 

## Clustercrit
library("clusterCrit")
intCriteria(traj = as.matrix(df_dum_scaled), part = sub_grp2, crit = c("Gamma", "Dunn"))

```

### Validate dendogram using cophenetic coefficient
```{r}
# Compute cophentic distance
res.coph <- cophenetic(as.hclust(hc_agnes_ward))

# Correlation between cophenetic distance and
# the original distance
cor(gower_asdist, res.coph)

# Compare w/ Single
res.hc2 <- hclust(as.dist(gower_dist), method = "average")

cor(as.dist(gower_dist), cophenetic(res.hc2))

# Compare w/ Average
res.hc3 <- hclust(as.dist(gower_dist), method = "single")

cor(as.dist(gower_dist), cophenetic(res.hc3))

# methods to assess
m <- c("average", "single", "complete", "weighted")
names(m) <- c("average", "single", "complete", "weighted")

# function to compute coefficient
gower_coph_cor <- function(x) {
  # res.hc3 <- hclust(as.dist(gower_dist), method = x)
  res.hc3 <- as.hclust(agnes(gower_dist, diss = TRUE, method = x))
  cor(as.dist(gower_dist), cophenetic(res.hc3))
}

map_dbl(m, coph_cor)
## average    single  complete  weighted 
## 0.5039878 0.2905799 0.4117255 0.4031402


# function to compute coefficient
manhattan_coph_cor <- function(x) {
  res.hc3 <- as.hclust(agnes(onehot_dist, diss = TRUE, method = x))
  cor(as.dist(gower_dist), cophenetic(res.hc3))
}

map_dbl(m, manhattan_coph_cor) 
## REALLY BAD RESULTS -- YIKES
##  average    single  complete  weighted 
## 0.2340356 0.1920028 0.2089606 0.2007872 

```

## BAD Run clustering algorithm 10 times to check for consistency

```{r}
library("mclust")

seeds <- numeric()
subgrps <- data.frame()

gow_dist <- daisy(df_clustervars[rows, ], metric = c("gower"))

for(i in seq(1:10)){
  set.seed(i)
  
  # # Shuffle rows
  # rows <- sample(nrow(df_clustervars))
  # gow_dist <- daisy(df_clustervars[rows, ], metric = c("gower"))
  # 
  # # Build distance matrix from shuffled df
  # 
  # # dend <- agnes(gow_dist, diss = TRUE, method = "complete")
  # dend <- hclust(as.dist(gow_dist), method = "complete")
  # 
  # # Cut tree
  # agnes_hclust <- as.hclust(dend) # get hclust() version of agnes()
  # sub_grp <- cutree(agnes_hclust, k = 2) # cut tree at 2 clusters
  # 
  # # Organize by value IDs
  # sub_grp_names <- as.numeric(names(sub_grp))
  # df_subgrp <- data.frame("names" = sub_grp_names, "value" = sub_grp)
  # df_subgrp <- df_subgrp[order(df_subgrp[, 1]), ]
  # sorted_sub_grp <- df_subgrp$value
  # names(sorted_sub_grp) <- df_subgrp$names
  # subgrps <- rbind(subgrps, sorted_sub_grp)
  
  data(mtcars)
  d <- dist(mtcars, method = "euclidean") # distance matrix
  fit <- hclust(d, method="ward.D") 
  groups <- cutree(fit, k=5) # cut tree into 5 clusters
  subgrps <- rbind(subgrps, groups)

}

subgrps <- as.data.frame(t(subgrps))

all_combos <- combn(1:10, 2)

all_adjrand <- numeric()

for(i in seq(1,ncol(all_combos))){
  classif_1 <- unlist(subgrps[all_combos[1, i]])
  classif_2 <- unlist(subgrps[all_combos[2, i]])
  adj_rand <- adjustedRandIndex(classif_1, classif_2)
  all_adjrand <- append(all_adjrand, adj_rand)
}

all_adjrand

```







### Visualizing and adding clusters to data
* We chose 4 groups based on above analysis

"The height of the cut to the dendrogram controls the number of clusters obtained. It plays the same role as the k in k-means clustering. In order to identify sub-groups (i.e. clusters), we can cut the dendrogram with cutree."

"We can also use the cutree output to add the the cluster each observation belongs to to our original data."

```{r}
# Cut tree into 4 groups
sub_grp4 <- cutree(as.hclust(hc_agnes_ward), k = 4)

# Number of members in each cluster
table(sub_grp4)
## sub_grp
##  1   2   3   4 
## 251 612 342 274  


# Cut tree into 2 groups
sub_grp2 <- cutree(as.hclust(hc_agnes_ward), k = 2)

# Number of members in each cluster
table(sub_grp2)
## sub_grp
##  1   2  
## 525 954  
```

#### Summarizing numeric

```{r}
# 2 groups
df_clustervars$cluster2 <- sub_grp2

# 4 groups
df_clustervars$cluster4 <- sub_grp4

```

```{r}
# 2 groups
df_clustervars %>% # excluding PIDP
  group_by(cluster2) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)

df_clustervars %>% # excluding PIDP
  group_by(cluster2) %>%
  summarise_if(is.numeric, median, na.rm = TRUE)
```

```{r}
# 4 df_interest_complete
df_clustervars %>%
  group_by(cluster4) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)

df_clustervars %>% # excluding PIDP
  group_by(cluster4) %>%
  summarise_if(is.numeric, median, na.rm = TRUE)

```

#### Summarizing categorical data
```{r}
factor_cols <- colnames(df_clustervars[,sapply(df_clustervars, is.factor) & colnames(df_clustervars) != c("pidp", "hidp")])

cat_data <- df_clustervars %>% 
  select(factor_cols)

for(i in seq(1:ncol(cat_data))){
  var <- colnames(cat_data)[i]
  column_data <- cat_data[[i]]
  print(colnames(cat_data[i]))
  print(prop.table(table(cluster = df_clustervars$cluster2, column_data), margin = 1))
}
```




# Divisive clustering
(According to Berkeley source above, good at identifying large clusters)


```{r}
divisive.clust <- diana(as.matrix(gower_dist), 
                  diss = TRUE, keep.diss = TRUE)

divisive.clust$dc

plot(divisive.clust, main = "Divisive")

```

# Stats: Agglom vs. Div

source: https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995
```{r}
library("fpc")

cstats.table <- function(dist, tree, k) {
clust.assess <- c("cluster.number","n","within.cluster.ss","average.within","average.between",
                  "wb.ratio","dunn2","avg.silwidth")
clust.size <- c("cluster.size")
stats.names <- c()
row.clust <- c()
output.stats <- matrix(ncol = k, nrow = length(clust.assess))
cluster.sizes <- matrix(ncol = k, nrow = k)
for(i in c(1:k)){
  row.clust[i] <- paste("Cluster-", i, " size")
}
for(i in c(2:k)){
  stats.names[i] <- paste("Test", i-1)
  
  for(j in seq_along(clust.assess)){
    output.stats[j, i] <- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.assess])[j]
    
  }
  
  for(d in 1:k) {
    cluster.sizes[d, i] <- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.size])[d]
    dim(cluster.sizes[d, i]) <- c(length(cluster.sizes[i]), 1)
    cluster.sizes[d, i]
    
  }
}
output.stats.df <- data.frame(output.stats)
cluster.sizes <- data.frame(cluster.sizes)
cluster.sizes[is.na(cluster.sizes)] <- 0
rows.all <- c(clust.assess, row.clust)
# rownames(output.stats.df) <- clust.assess
output <- rbind(output.stats.df, cluster.sizes)[ ,-1]
colnames(output) <- stats.names[2:k]
rownames(output) <- rows.all
is.num <- sapply(output, is.numeric)
output[is.num] <- lapply(output[is.num], round, 2)
output
}
# I am capping the maximum amout of clusters by 7
# I want to choose a reasonable number, based on which I will be able to see basic differences between customer groups as a result
stats.df.divisive <- cstats.table(gower_dist, divisive.clust, 7)


# Stats for agglomerative
stats.agglom <- cstats.table(gower_dist, hc_agnes_ward, 7)

stats.df.divisive
stats.agglom

```

# K-medoids - PAM
source: https://uc-r.github.io/kmeans_clustering#silo

```{r}
library("tidyverse")  # data manipulation
library("cluster")    # clustering algorithms
library("factoextra") # clustering algorithms & visualization
```

## Visualize distance matrix
source: https://uc-r.github.io/kmeans_clustering#silo
```{r}
fviz_dist(gower_dist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```

## PAM - try many k's
```{r}
asw <- numeric(15)

for(i in 2:15){
  asw[i] <- pam(gower_dist, diss = TRUE, k = i) $ silinfo $ avg.width
}
# max is 0.09192858, where there are 4 clusters

k.best <- which.max(asw) + 1
cat("silhouette-optimal number of clusters:", k.best, "\n")

# Plot of silhouette
plot(2:16, asw, type= "b", main = "pam() clustering assessment",
     xlab= "k  (# clusters)", ylab = "average silhouette width")
axis(1, k.best, paste("best",k.best,sep="\n"), col = "red", col.axis = "red")

```



## optimal PAM
```{r}
# Using k = 4 since that had max silhouette coefficient
pam_1 <-  pam(gower_dist, diss = TRUE, k = 4)


```





